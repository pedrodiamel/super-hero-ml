{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1aed1d3b61a417be49e9c4cec0322e4c1b128d2a"
      },
      "cell_type": "markdown",
      "source": "# Challenge ML - Superheroes!!!!"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns  # visualization tool\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport missingno as msno\nsns.set()\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "# DATA EXPLORATION AND ANALYSIS\n## We explore the super powers. "
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "83dc3da77516bb217ec9e6664d42a5f1bf8f0ac0"
      },
      "cell_type": "code",
      "source": "data_hero = pd.read_csv('../input/super_hero_powers.csv')\ndata_hero.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eddcbcf700ee47e8ffa014dbc992477727c29cc6"
      },
      "cell_type": "markdown",
      "source": "## How is the distribution of the super powers?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30f6ebb0cd6d27d7662069e97c935c5b6fcdc96f"
      },
      "cell_type": "code",
      "source": "target_counts = data_hero.drop([\"hero_names\"],axis=1).sum(axis=0).sort_values(ascending=False)\nplt.figure(figsize=(15,25))\nsns.barplot(y=target_counts.index.values, x=target_counts.values, order=target_counts.index)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "df3096558c4bf457fc118439b472eb8c57775d75"
      },
      "cell_type": "markdown",
      "source": "## Which is the relationship of the super powers?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bedeb926d5d7d306bb2026774adcaecd636b12bb"
      },
      "cell_type": "code",
      "source": "# correlation mat \ndata_hero_corr = data_hero.drop([\"hero_names\"], axis=1).corr()\nplt.figure(figsize=(15,15))\nsns.heatmap( data_hero_corr , cmap=\"RdYlBu\", vmin=-1, vmax=1)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "48a8f2d3906b32e218ae7bccdaf58a1f2f246770"
      },
      "cell_type": "markdown",
      "source": "## How can we see, some super powers are correlated. \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d12d699aebecaf1985b8713384b09f53161ba807"
      },
      "cell_type": "code",
      "source": "# top > 0.65 correlation super powers \npowers = data_hero.drop(['hero_names'], axis=1)\nfor p in powers.columns:\n    top5 = data_hero_corr[p].abs().drop(p).nlargest(5)\n    top5 = top5[top5>0.65]\n    if top5.empty:\n        continue\n    for i,v in zip(top5.index, top5.values): \n        c = data_hero_corr[p][i]\n        print('{:30}\\t| {:30}\\t| {:<30}\\t| {:<30}'.format(p, i, v, c) )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e515d2456a2814b9d1e971a96d9ca682dbc2214d"
      },
      "cell_type": "markdown",
      "source": "## Which superheroes are more powerful?"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2d5ce790c89640bdab379604d081f68bc6c528d"
      },
      "cell_type": "code",
      "source": "# powerful (sum_{super powers}) by super heroe\ndata_hero_plus_power = data_hero\ndata_hero_plus_power[\"Powerful\"] = data_hero_plus_power.drop([\"hero_names\"],axis=1).sum(axis=1)\ndata_hero_sort = data_hero_plus_power.sort_values(by=['Powerful'], ascending=False)\nhero_names = data_hero_sort['hero_names'][:100]\npowerful   = data_hero_sort['Powerful'][:100]\n\nplt.figure(figsize=(15,20))\nsns.barplot(y=hero_names, x=powerful) \nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d70a0a12ce33ee54190800891b9ea49b09d490de"
      },
      "cell_type": "markdown",
      "source": "## We explore the some information about of the super powers."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10cabdbfe1060ea6c4e34d452810f08a889d01e0"
      },
      "cell_type": "code",
      "source": "data_info = pd.read_csv('../input/heroes_information.csv', index_col='Unnamed: 0',na_values='-')\ndata_info.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e18c280a32fa89cdeef7d3f76d58a7cb4390d2ce"
      },
      "cell_type": "code",
      "source": "data_info.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "091db7180c2ee7075de0e9f2c5e6ba32371144d7"
      },
      "cell_type": "markdown",
      "source": "## Missing values exploration"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "ca82272b5e806a5e2b0030d2ad604784b792a0c3"
      },
      "cell_type": "code",
      "source": "msno.matrix(data_info)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3caa0100ef8f081c113414ede83ab1f8df9c4dd"
      },
      "cell_type": "code",
      "source": "missing_data = data_info.isnull().sum().sort_values(ascending=False)\nplt.figure(figsize=(8,8))\nsns.barplot(y=missing_data.index.values, x=missing_data.values, order=missing_data.index)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "00944c8d3018d93929f9d38a22206ee211f5adb9"
      },
      "cell_type": "markdown",
      "source": "# Super powers \n## TSNE visialization of the data information "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2d278a99f9bf2bf14ca6205064414e1b69fe9820"
      },
      "cell_type": "code",
      "source": "from sklearn.manifold import TSNE\ncmap = plt.get_cmap('jet_r')\n\ndata_hero = pd.read_csv('../input/super_hero_powers.csv', na_values='-')\nZ = TSNE(n_components=2, init='pca', \n    random_state=0, perplexity=30).fit_transform( data_hero.drop(['hero_names'], axis=1))\n\nplt.figure(figsize=(12,12))\nplt.scatter(Z[:,0], Z[:,1], s=(20,20), marker='o', color=[0,0,1] );\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9af2f24c3583b8ae362137956a81ac65e783a0c"
      },
      "cell_type": "markdown",
      "source": "\n# Preprocessing data information of the superheros\n## Features description\n---\n\n\n| Feature       |  Types                                           |  Missing value        |  Observation                                                                                            |\n|-----------------|-------------------------------------------|-------------------------|-------------------------------------------------------------------------------------------|\n| Gender        |  [Male, Female]_2                        |  3.95%                    | <- random                                                                                                |\n| Eye color     |  ['yellow', ...]_22                            |  23.40%                  | <- non color                                                                                             |\n| Hair color     |  ['No Hair', 'Black',...]_29              |  23.43%                  | <- none                                                                                                    |\n| Skin color     |  ['blue', ...]_16                              |  90.19%                  | <- delete                                                                                                  |\n| Publisher      |  ['Marvel Comics', ...]_24             |  2.04%                    | <- none                                                                                                    |\n| Race             |  ['Human', 'Icthyo Sapien',]_61   |  41.41%                  | <- non human                                                                                          |         \n| Alignment      |  ['good', 'bad', 'neutral']_3          |  0.95%                    | <- neutral (in the real case we could go to search the missing label )     |\n| Height           |  real value                                   | 0.27%                    | <- mean of the values                                                                             |\n| Weight          |  real value                                   | 0.00%                    | -                                                                                                              |\n\n\n\nThe **Skin color** have 90% of the missing value. This feature is not include in the processing.\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4d77f2b4d77dc8b37eb69b52bf97351212bf0243"
      },
      "cell_type": "code",
      "source": "# Aux function \n\nimport unicodedata\nimport re\n\n# Turn a Unicode string to plain ASCII, thunicodedata\n# http://stackoverflow.com/a/518232/280942unicodedata\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s\n\n# Filter string features \ndef filterFeature( X, strpref='' ):\n    for i,x in enumerate(X):\n        if X.isnull()[i]: \n            continue\n        X[i] = strpref + normalizeString(x)   \n    return X\n\n# Sensitive cases errors of the color name   \ndef filterCaseColor(s):\n    if s == 'yellow without irises ': s ='yellow'\n    if s == 'no hair': s='none'\n    if s == 'strawberry blond': s='blond'\n    if s == 'bown': s='brown'\n    if s == 'brownn': s='brown'\n    return s\n\n# Filter color (union of the colors space)\ndef filterColor(colors):\n    mapcolor={ c:filterCaseColor(normalizeString(c)).split()  for c in colors }\n    colors=[]\n    for k,v in mapcolor.items():\n        colors.append(v)\n    colors = np.unique(np.concatenate( colors, axis=0 ))\n    colors = { c:i for i,c in enumerate(colors)  }    \n    return mapcolor, colors\n\n# Vector to one hot representation \ndef v2hot(v,n):\n    hot = np.zeros(n); hot[v]=1\n    return hot\n\ndef color2OneHot( colors, mapcolor, strpref='' ):\n    colors_hot = [[] for _ in colors ]\n    for i,c in enumerate(colors):\n        if colors.isnull()[i]:continue\n        colors_hot[i] = mapcolor[c]\n        \n    colors_uq = np.unique(np.concatenate(colors_hot,axis=0))\n    n = len(colors_uq)\n    colors_map = { c:i for i,c in enumerate(colors_uq)  } \n    colors_hot = [ v2hot([ colors_map[c] for c in cs ], n) for cs in colors_hot   ]\n    colors_hot = np.stack(colors_hot, axis=0)\n    colors_uq = np.array([ strpref + c for c in colors_uq ] )\n    colors_df = pd.DataFrame( data=colors_hot, columns=colors_uq )\n    return colors_df\n\ndef norm(x):\n    x = (x - np.mean(x))/np.std(x)\n    return x\n\n# visualization \ndef barplot( df, fsize=(8,8) ):\n    df_sum = df.sum( axis=0 ).sort_values(ascending=False)\n    plt.figure(figsize=fsize)\n    sns.barplot(y=df_sum.index.values, x=df_sum.values, order=df_sum.index)\n    plt.show()\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "173a54a3e9b93d3280e81184a7b0eb4aae825996"
      },
      "cell_type": "markdown",
      "source": "## Filter Colors and Analysis\n\nThe Eye color, Hair color and Skin color represent the same type fo space color. In this point we are normalizate of value and filters. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "62ff9e3c3bc5b1d9531b599fc72d86cb97156f1f"
      },
      "cell_type": "code",
      "source": "info_colors = ['Eye color', 'Hair color', 'Skin color' ]\ncolors = [ data_info[info_color][~data_info[info_color].isnull()].unique() for info_color in info_colors]\ncolors = np.concatenate(colors,axis=0)\nmapcolor, colors = filterColor( colors )\nprint(colors)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "15fe4b1ba7c0197e10e84e172033e0fabe644d21"
      },
      "cell_type": "markdown",
      "source": "## Eye color analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c1979ec4ab33ac0d15be741811aa2a1550a566a0"
      },
      "cell_type": "code",
      "source": "eye_colors = data_info['Eye color']\neye_colors_df = color2OneHot(eye_colors, mapcolor)\nprint(eye_colors_df.head())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3ff89744a45238e663178911497569710c74ea4a"
      },
      "cell_type": "code",
      "source": "barplot(eye_colors_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0498684075460fab846083c47cc324fff228224b"
      },
      "cell_type": "markdown",
      "source": "## Hair color analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "712c9c6589c6a2698fad6beff4ba5f5225e7e532"
      },
      "cell_type": "code",
      "source": "hair_colors = data_info['Hair color']\nhair_colors_df = color2OneHot(hair_colors, mapcolor, 'hair_')\nprint(hair_colors_df.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "65a6898bcf06e1465f0082fb70b5f5cf629b1c5c"
      },
      "cell_type": "code",
      "source": "barplot(hair_colors_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c34283e60efdbc6faaf0c83eaa34262e60081fd5"
      },
      "cell_type": "markdown",
      "source": "## Race analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de861449b3340f99fc717bf067e76e83b7cd7ab1"
      },
      "cell_type": "code",
      "source": "race = data_info['Race'].copy()\nrace = filterFeature( race, 'race_' ) \nrace[race.isnull()] = 'race_no_human'\nrace_df = pd.get_dummies(race)\nrace_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3722b5ae523ce359788d6fc582edd6b15f402075"
      },
      "cell_type": "code",
      "source": "barplot(race_df, (8,14))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6faa0c6b79d98587632b94e616e1130746f94e1b"
      },
      "cell_type": "markdown",
      "source": "## Publisher analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85394d117f766836fdc608dccb4331faca6b38b2"
      },
      "cell_type": "code",
      "source": "publisher = data_info['Publisher'].copy()\npublisher = filterFeature( publisher, 'publisher_' ) \npublisher[publisher.isnull()] = 'publisher_none'\npublisher_df = pd.get_dummies(publisher)\npublisher_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "92d667847c7780d9f3d7d6d72786bb96e4b113bf"
      },
      "cell_type": "code",
      "source": "barplot(publisher_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81fae1d84fe80320d5929ddc827014b4e413085f"
      },
      "cell_type": "markdown",
      "source": "## Alignment analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "00f85fb82ab8f81cbd98dc09b981ce108e7013e7"
      },
      "cell_type": "code",
      "source": "alignment = data_info['Alignment'].copy()\nalignment = filterFeature( alignment, 'alig_' ) \nalignment[alignment.isnull()] = 'alig_neutral'\nalignment_df = pd.get_dummies(alignment)\nalignment_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a3f87a9ef8496d72fcbcd2d3770d85dd21293c9"
      },
      "cell_type": "code",
      "source": "barplot(alignment_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7748c94650f4d045b1ea65dda095393735b1ad02"
      },
      "cell_type": "markdown",
      "source": "## Height analysis"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31024ed22223702723304e737745616d51791a7c"
      },
      "cell_type": "code",
      "source": "data_info['Height'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6bc0a17151757b028f49146eb098d4b247682dcb"
      },
      "cell_type": "code",
      "source": "height = data_info['Height'].copy()\nheight[data_info['Height'].isnull()] = height[~data_info['Height'].isnull()].mean()\nheight_df =  pd.DataFrame( data=norm(height))\n\nplt.figure( figsize=(14,6))\nplt.subplot(121)\nplt.hist( norm(height), bins=50,  density=True, facecolor='g', alpha=0.75)\nplt.subplot(122)\nplt.boxplot( norm(height))\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3bfba9fe52c1aec9813822ea672bb1d30487a221"
      },
      "cell_type": "markdown",
      "source": "## Weight analysis "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff6370494b7599146d1e0639d1a4280db277f899"
      },
      "cell_type": "code",
      "source": "data_info['Weight'].describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68d3e61af5704c8f23ebb0db1c492fd74c564a40"
      },
      "cell_type": "code",
      "source": "weight = data_info['Weight'].copy()\nweight[data_info['Weight'].isnull()] = 0\nweight_df =  pd.DataFrame( data=norm(weight))\n\nplt.figure( figsize=(14,6))\nplt.subplot(121)\nplt.hist( norm(weight), bins=50,  density=True, facecolor='g', alpha=0.75)\nplt.subplot(122)\nplt.boxplot( norm(weight))\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "541ed5bc5a0c0ab65614fda2e0e9e2393d66803e"
      },
      "cell_type": "markdown",
      "source": "# Preprocessing data information of the superheroes "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6895ff1cbc95bb4ba829244a01b5d7edf600548b",
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# preprocessing pipeline of the data information \ndef prepocessing( data_info ):\n    data_info_prep = data_info.copy()\n\n    # color prepo\n    info_colors = ['Eye color', 'Hair color', 'Skin color' ]\n    colors = [ data_info[info_color][~data_info[info_color].isnull()].unique() for info_color in info_colors]\n    colors = np.concatenate(colors,axis=0)\n    mapcolor, colors = filterColor( colors )\n\n    # Eye color analysis\n    eye_colors = data_info['Eye color'].copy()\n    eye_colors_df = color2OneHot(eye_colors, mapcolor, 'eye_')\n\n    # Hair color analysis\n    hair_colors = data_info['Hair color'].copy()\n    hair_colors_df = color2OneHot(hair_colors, mapcolor, 'hair_')\n\n    # Race analysis\n    race = data_info['Race'].copy()\n    race = filterFeature( race, 'race_' ) \n    race[race.isnull()] = 'race_no_human'\n    race_df = pd.get_dummies(race)\n\n    # Publisher anaysis \n    publisher = data_info['Publisher'].copy()\n    publisher = filterFeature( publisher, 'pub_' ) \n    publisher[publisher.isnull()] = 'pub_none'\n    publisher_df = pd.get_dummies(publisher)\n\n    # Alignmant analysis \n    alignment = data_info['Alignment'].copy()\n    alignment = filterFeature( alignment, 'alig_' ) \n    alignment[alignment.isnull()] = 'alig_neutral'\n    alignment_df = pd.get_dummies(alignment)\n\n    # Height\n    height = data_info['Height'].copy()\n    height[data_info['Height'].isnull()] = 0\n    height_df =  pd.DataFrame( data=norm(height))\n\n    # Weight\n    weight = data_info['Weight'].copy()\n    weight[data_info['Weight'].isnull()] = 0\n    weight_df =  pd.DataFrame( data=norm(weight))\n    \n    # Gender\n    gender = data_info['Gender'].copy()\n    gender_df = (gender=='Male')*1\n    gender_df =  pd.DataFrame( data=gender_df )\n        \n    # Create dataframe \n    # eye_colors_df, hair_colors_df, race_df, publisher_df, alignment_df, height_df, weight_df, gender_df\n    return pd.concat([\n        eye_colors_df,\n        hair_colors_df,\n        race_df, \n        publisher_df, \n        alignment_df, \n        height_df, \n        weight_df, \n        gender_df\n    ],  axis=1, sort=False)\n\n# load data and preprocessing \ndata_info = pd.read_csv('../input/heroes_information.csv', index_col='Unnamed: 0',na_values='-')\nnew_data_info = prepocessing( data_info )\nnew_data_info.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "98b1c1472958133c34ed14159392118d36b2c47a"
      },
      "cell_type": "markdown",
      "source": "## TSNE visialization of the data information "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f00a5e641a655473e02ea1b4a4220573b12797b"
      },
      "cell_type": "code",
      "source": "from sklearn.manifold import TSNE\nZ = TSNE(n_components=2, init='pca', \n    random_state=0, perplexity=30).fit_transform( new_data_info )\n\nplt.figure(figsize=(12,12))\nplt.scatter(Z[:,0], Z[:,1], s=(20,20), marker='o', color=[1,0,0] );\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b61d6bd5e1b853663e5c11c5920b6dea5a37bbe"
      },
      "cell_type": "markdown",
      "source": "# Create dataset for superheroe !!!!"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77fbb72619808b9320c29057b747a8e2b999b7a0"
      },
      "cell_type": "code",
      "source": "\n# load data and preprocessing \ndata_info = pd.read_csv('../input/heroes_information.csv', index_col='Unnamed: 0',na_values='-')\ndata_hero = pd.read_csv('../input/super_hero_powers.csv', na_values='-')\n\n# preprocessiong\nnew_data_info = prepocessing( data_info )\n\n# union of information \nname_info = filterFeature(data_info['name'].copy())\nname_hero = filterFeature(data_hero['hero_names'].copy())\n\ntuplas=[]\nfor i in range(len(name_hero)):\n    for j in range(len(name_info)):\n        if name_hero[i] == name_info[j]:\n            tuplas.append( pd.concat([new_data_info.iloc[j], data_hero.drop(['hero_names'], axis=1).iloc[i]], axis=0, sort=False) )\n\n# data_processes = pd.DataFrame( data=tuplas )\ndata_processes = pd.concat(tuplas, axis=1, sort=False).T\n\ndata_processes.to_csv( './data_processes.csv' , index=False, encoding='utf-8')\ndata_processes.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "587529b774d4b49bfc23521eaadf335a7c8506a9"
      },
      "cell_type": "code",
      "source": "Z = TSNE(n_components=2, init='pca', \n    random_state=0, perplexity=30).fit_transform( data_processes )\n\nplt.figure(figsize=(12,12))\nplt.scatter(Z[:,0], Z[:,1], s=(20,20), marker='o', color=[1,0,1] );\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43a0a53ffa049ad4228f3c2b5c266310afdcb2dd"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "104060810e3fef27cec8ae3f9c9beb95e67fb2a6"
      },
      "cell_type": "markdown",
      "source": "# Machine Learning Challenge\nThis is a small series of exercises to evaluate your knowledge in machine learning. Please respond the questions detailing the steps you took to solve each task. All questions are simple, but this is your chance to show us your technical knowledge, so do not hold out on your math skills."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "670f5e7c9aeefab543aae049bf2acae26dd6a874"
      },
      "cell_type": "markdown",
      "source": "## Clustering\n\n### Question 1\nFirst, we want to cluster our superheroes according to both their powers and information. Run an unsupervised clustering method using the number of clusters that you judge the most appropriate.\n1. Which algorithm did you pick and why?\n2. Which features did you use and why? Please explain any pre-processing or feature engineering (selection) you have performed."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d3b2311246df43becd1aab74faef30bb0bf563bd"
      },
      "cell_type": "markdown",
      "source": "### Answer 1\n\nFor simplicity visual analysis and interpretation of the result of clustering, we used the super powers information. The study could be extended for all features.  The super heroes powers features is given by a binary, discrete value: 0 for absence; 1 for presence. As I like to find groupings or clusters of these discrete quantities I liked to use a discrete clustering algorithm. We can used of mixture models, in particular a Bernoulli mixture model.\n\n\n\n### **Bernoulli mixture model **\n\n**Objetive funtion:**\n$$\\ln p(X,Z|\\mu, \\pi) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\gamma (z_{n,k}) \\left( \\ln \\pi_{k} + \\sum_{d=1}^{D} x_{n,d} \\ln \\mu_{k,d} + (1- x_{n,d}) \\ln (1-\\mu_{k,d}) \\right)$$\n\n**E-Step: responsibilites:**\n\n$$\\gamma( z_{n,k} ) = \\frac{\\pi_{k} p(x_{n}|\\mu_{k})} {\\sum_{j=1}^{K} \\pi_{j} p(x_{n}|\\mu_{j})}$$\n\n**M-Step: maximization:**\n$$N_{k} = \\sum_{n=1}^{N} \\gamma(z_{n,k})$$\n$$\\mu_{k} = \\frac{1}{N_{k}} \\sum_{n=1}^{N} \\gamma(z_{n,k}) x_{n}$$\n$$\\pi_{k} = \\frac{N_{k}}{N}$$\n\n"
    },
    {
      "metadata": {
        "_uuid": "3fb3dcdfe6d82f06cf2dfea20f8f14308675d1e6"
      },
      "cell_type": "markdown",
      "source": "## Implementation model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1149ef333ef42c43ce64b61ed86bc96a47dbfc2"
      },
      "cell_type": "code",
      "source": "from scipy.special import logsumexp\n\nclass BernoulliMixture:    \n    def __init__(self, n_components, max_iter, tol=1e-3):\n        self.n_components = n_components\n        self.max_iter = max_iter\n        self.tol = tol\n    \n    def fit(self,x):\n        self.x = x\n        self.init_params()\n        log_bernoullis = self.get_log_bernoullis(self.x)\n        self.old_logL = self.get_log_likelihood(log_bernoullis)\n        for step in range(self.max_iter):\n            if step > 0:\n                self.old_logL = self.logL            \n            # E-Step\n            self.gamma = self.get_responsibilities(log_bernoullis)\n            self.remember_params()\n            # M-Step\n            self.get_Neff()\n            self.get_mu()\n            self.get_pi()\n            # Compute new log_likelihood:\n            log_bernoullis = self.get_log_bernoullis(self.x)\n            self.logL = self.get_log_likelihood(log_bernoullis)            \n            if np.isnan(self.logL):\n                self.reset_params()\n                print(self.logL)\n                break\n\n    def reset_params(self):\n        self.mu = self.old_mu.copy()\n        self.pi = self.old_pi.copy()\n        self.gamma = self.old_gamma.copy()\n        self.get_Neff()\n        log_bernoullis = self.get_log_bernoullis(self.x)\n        self.logL = self.get_log_likelihood(log_bernoullis)\n        \n    def remember_params(self):\n        self.old_mu = self.mu.copy()\n        self.old_pi = self.pi.copy()\n        self.old_gamma = self.gamma.copy()\n    \n    def init_params(self):\n        self.n_samples = self.x.shape[0]\n        self.n_features = self.x.shape[1]\n        #self.gamma = np.zeros(shape=(self.n_samples, self.n_components))\n        self.pi = 1/self.n_components * np.ones(self.n_components)\n        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n        self.normalize_mu()\n    \n    def normalize_mu(self):\n        sum_over_features = np.sum(self.mu, axis=1)\n        for k in range(self.n_components):\n            self.mu[k,:] /= sum_over_features[k]\n            \n    def get_responsibilities(self, log_bernoullis):\n        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n        for k in range(self.n_components):\n            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n        return gamma\n        \n    def get_log_bernoullis(self, x):\n        log_bernoullis = self.get_save_single(x, self.mu)\n        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n        return log_bernoullis\n    \n    def get_save_single(self, x, mu):\n        mu_place = np.where(np.max(mu, axis=0) <= 1e-15, 1e-15, mu)\n        return np.tensordot(x, np.log(mu_place), (1,1))\n        \n    def get_Neff(self):\n        self.Neff = np.sum(self.gamma, axis=0)\n    \n    def get_mu(self):\n        self.mu = np.einsum('ik,id -> kd', self.gamma, self.x) / self.Neff[:,None] \n        \n    def get_pi(self):\n        self.pi = self.Neff / self.n_samples\n    \n    def predict(self, x):\n        log_bernoullis = self.get_log_bernoullis(x)\n        gamma = self.get_responsibilities(log_bernoullis)\n        return np.argmax(gamma, axis=1)\n        \n    def get_sample_log_likelihood(self, log_bernoullis):\n        return logsumexp(np.log( self.pi[None,:] ) + log_bernoullis, axis=1)\n    \n    def get_log_likelihood(self, log_bernoullis):\n        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n        \n    def score(self, x):\n        log_bernoullis = self.get_log_bernoullis(x)\n        return self.get_log_likelihood(log_bernoullis)\n    \n    def score_samples(self, x):\n        log_bernoullis = self.get_log_bernoullis(x)\n        return self.get_sample_log_likelihood(log_bernoullis)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6c83b3731f92d113434d51d86629203935c27575"
      },
      "cell_type": "markdown",
      "source": "## Clustering \n### Question 2\n\nOne of the challenges in clustering is defining the right number of clusters. How did you choose that number?\n\nI used the Elbow method for select the number of clustering.\n\nHow do you evaluate the quality of the final clusters?\n\nFor evaluate we used the cross validation.\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eabd8fcb877a5debc88209950e84f5689b9ec724"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX = pd.read_csv('../input/super_hero_powers.csv', na_values='-').drop( 'hero_names', axis=1 )*1\nX = np.array(X, dtype=np.int)\nclusters = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50] \nx_train, x_test = train_test_split(X, shuffle=True, random_state=0)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ab0ed46ca1c3e21ebbd3d55123b2ebf1876471c9"
      },
      "cell_type": "code",
      "source": "scores = []\nfor n in range(len(clusters)):\n    model = BernoulliMixture(clusters[n], 200) \n    model.fit(x_train)\n    score = model.score(x_test)\n    scores.append(score)\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aaa3a94751c9e98dc987c7edd9cac1b610b97b27"
      },
      "cell_type": "code",
      "source": "est_clusters = clusters[np.argmin(scores)]\nplt.figure()\nplt.plot(clusters, scores)\nplt.plot(est_clusters, scores[np.argmin(scores)],'or')\nplt.title('Elbow method')\nplt.ylabel('J(x)')\nplt.xlabel('Number of clusters')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7282b8178ec56175a1854cbe2db933a28a550766"
      },
      "cell_type": "code",
      "source": "model = BernoulliMixture(est_clusters, 200)\nmodel.fit(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3eb8d599251e2ffac53cd4b2ff7fa21fb506859d"
      },
      "cell_type": "code",
      "source": "results = pd.read_csv('../input/super_hero_powers.csv', na_values='-').drop( 'hero_names', axis=1 )*1\nresults[\"cluster\"] = np.argmax(model.gamma, axis=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d90a778262925c3fcb2ef9d1c8482d646c019fc"
      },
      "cell_type": "markdown",
      "source": "## Visual analysis of the results"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "925e97f3e762ea495c31bc7d01006d577f471a1a"
      },
      "cell_type": "code",
      "source": "G = results.groupby(\"cluster\").sum() / results.drop(\"cluster\", axis=1).sum(axis=0) * 100\nG = G.apply(np.round).astype(np.int32)\n\nplt.figure(figsize=(40,5))\nsns.heatmap(G, cmap=\"Oranges\", annot=True, fmt=\"g\", cbar=False, annot_kws={\"size\": 6});\nplt.title(\"How are specific super power over clusters in percent?\");",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2fd765af1921482006069f66e88e82d2300d5a60"
      },
      "cell_type": "markdown",
      "source": "## We can define new features based on the results obtained."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73384ee2bae37b862f0651abd06df2affcc7e6cb"
      },
      "cell_type": "code",
      "source": "for g in np.array(G):\n    gi = np.argsort(g)[::-1]\n    #top = np.where(np.array(g)>75)[0]\n    name_list = np.array(results.columns[ gi ][:3] )\n    print('{}/{}/{}'.format(name_list[0],name_list[1],name_list[2] ))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16b1153069f52721aec42ee4796b98c65ba25106"
      },
      "cell_type": "code",
      "source": "from sklearn.manifold import TSNE\ncmap = plt.get_cmap('jet_r')\n\nX = pd.read_csv('../input/super_hero_powers.csv', na_values='-').drop( 'hero_names', axis=1 )*1\nY = np.argmax(model.gamma, axis=1)\nnameG = [\n    'Qwardian Power Ring/Speed Force/Power Cosmic',\n    'Spatial Awareness/Anti-Gravity/Hyperkinesis',\n    'Echolocation/Wallcrawling/Web Creation',\n    'Intuitive aptitude/Hair Manipulation/Illumination',\n    'Vision - Cryo/Vision - Microscopic/Vision - Heat',\n    'Terrakinesis/Weather Control/Water Control',\n    'Omniscient/Banish/Astral Travel',\n    'The Force/Cloaking/Mind Control Resistance',\n    'Toxin and Disease Resistance/Elasticity/Magic Resistance',\n    'Changing Armor/Photographic Reflexes/Peak Human Condition'\n        ]\n\nZ = TSNE(n_components=2, init='pca', \n    random_state=0, perplexity=50).fit_transform(X)\n\n#show\nplt.figure( figsize=(12,12) )\n#plt.scatter(Xt[:,0], Xt[:,1], s=(10,10), marker='o', c=Yo);\nn = len(np.unique(Y))\nfor i in range( n ):\n    index = Y==i\n    color = cmap(float(i)/n) \n    plt.scatter(Z[index,0], Z[index,1], s=(20,20), marker='o', color=color, label='{}'.format( nameG[i] ) ); #dataloader.dataset.data.classes[i]\n\nplt.legend()\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "34f1468a42867424bff85a956c11baed8daa8e98"
      },
      "cell_type": "markdown",
      "source": "## Spotting the Bad Guys\nIn this section, we will deal with the supervised learning problem. More concretely, we will formulate a classification task, and our target is the super-heroes alignment (good or bad).\n\n### Question 3\nFirst, we will use the Naive Bayes algorithm. Run the algorithm on the superheroes data to predict the alignment variable and evaluate the results. Again,\nplease detail any pre-processing and feature engineering you applied in the process.\n\n1. Which hypotheses do we assume when using the Naive Bayes algorithm?\n2. How do the specific characteristics of this dataset influence your modeling choices and results?\n3. How do you evaluate the results?\n"
    },
    {
      "metadata": {
        "_uuid": "7d547e5f478e7fa2536f096c90ef127e00e4be82"
      },
      "cell_type": "markdown",
      "source": "### Answer 3\n\n\n**Naive Bayes algorithm**\n\n1. Which hypotheses do we assume when using the Naive Bayes algorithm?\n\n    The likelihood of the features is assumed to be Gaussian:\n$$P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)$$\n\n2. How do the specific characteristics of this dataset influence your modeling choices and results?\n\n    The Naive Bayes assume the Gaussian distribution of the features. The categorical data have not Gaussian distribution.  \n\n3. How do you evaluate the results?\n\n    KFold Cross Validation\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f4005890bf1c86fbb8407cad1dc379d12dd1e89"
      },
      "cell_type": "code",
      "source": "#load prep dataset\ndata_prep = pd.read_csv('./data_processes.csv')\ndata_prep.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f467d17c92a4ba7abd9a8b98f13d18c6b797629"
      },
      "cell_type": "code",
      "source": "\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.metrics as metrics\n\nX = np.array(data_prep.drop( ['alig_bad', 'alig_good', 'alig_neutral'], axis=1 ).copy())\nY = np.array(data_prep[ ['alig_bad', 'alig_good', 'alig_neutral'] ].copy())\n\nX = X[Y[:,2]==0,:]              #delete neutral in x\nY = Y[Y[:,2]==0,:2]             #delete neutral in y\nY = (Y[:,0] + Y[:,1]*2) - 1     #hot2val\n\n# Leave-One-Out cross-validator\n# X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.33, shuffle=True, random_state=0)\n\n# 10Fold Cross Validation\nresult=[]\nk=10\nkf = StratifiedKFold(n_splits=k)\nkf.get_n_splits(X, Y)\n\nclf = GaussianNB()\n# clf = BernoulliNB()\n# clf = RandomForestClassifier(n_estimators=300, oob_score=True, random_state=123456)\nprint('KFold\\t|Acc\\t|Prec\\t|Rec\\t|F1\\t|')\nfor i,(train_index, test_index) in enumerate(kf.split(X,Y)):\n    \n    X_train = X[train_index,:]; X_test = X[test_index,:]\n    y_train = Y[train_index  ]; y_test = Y[test_index  ]\n        \n    # Estimate\n    clf.fit(X_train,y_train)\n    # Predict\n    y_test_hat = clf.predict(X_test)\n\n    # Evaluate\n    acc = metrics.accuracy_score(y_test, y_test_hat)\n    precision = metrics.precision_score(y_test, y_test_hat, average='macro')\n    recall = metrics.recall_score(y_test, y_test_hat, average='macro')\n    f1_score = 2*precision*recall/(precision+recall)\n\n    result.append([acc,precision,recall,f1_score])  \n    print( 'K({}):\\t|{:0.3f}\\t|{:0.3f}\\t|{:0.3f}\\t|{:0.3f}\\t|'.format(i,acc,precision,recall,f1_score).replace('.',',')  )\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db267d017c7bb4882871647c4213bd5f1cd295eb"
      },
      "cell_type": "code",
      "source": "result_mat = np.stack(result, axis=1).T\nmeanNB = result_mat.mean(axis=0)\nstdNB  = result_mat.std(axis=0)\n\nplt.figure(figsize=(12,5))\nind = np.arange(4) \nplt.bar(ind, meanNB, 0.35, yerr=stdNB, color='red')\nplt.ylabel('Scores')\nplt.title('Scores by group and gender')\nplt.xticks(ind, ('Acc', 'Prec', 'Rec', 'F1'))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d6844ccc0d1f06db60133be7f5173795b91666c2"
      },
      "cell_type": "markdown",
      "source": "### Question 4\n\nNow feel free to run the classification algorithm that you judge the most appropriate for this task.\n\n1. What motivated your choice of algorithm?\n2. How does this algorithm compare with the Naive Bayes regarding modeling assumptions and results?\n\n"
    },
    {
      "metadata": {
        "_uuid": "fc53a470638a14a7db87631b30b638346e1ac462"
      },
      "cell_type": "markdown",
      "source": "### Answare\n\n1. What motivated your choice of algorithm?\n\nFor categorical value and the distribution of the data I select the following methods:\n\n - Random Forest \n - Bernoulli Naive Bayes\n     \n2. How does this algorithm compare with the Naive Bayes regarding modeling assumptions and results?\n\nI used KFold Cross-Validation. The results show that the Random Forest is the best.  In this case, the visual results are sufficient to demonstrate the significance statistical ( Random Forest  vs  Naive Bayes). \n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "745fcd965cf0afedfa6a06d4650336ecd5cefea7"
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.metrics as metrics\n\nX = np.array(data_prep.drop( ['alig_bad', 'alig_good', 'alig_neutral'], axis=1 ).copy())\nY = np.array(data_prep[ ['alig_bad', 'alig_good', 'alig_neutral'] ].copy())\n\nX = X[Y[:,2]==0,:]              #delete neutral in x\nY = Y[Y[:,2]==0,:2]             #delete neutral in y\nY = (Y[:,0] + Y[:,1]*2) - 1     #hot2val\n\n# Leave-One-Out cross-validator\n# X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.33, shuffle=True, random_state=0)\n\n# 10Fold Cross Validation\n\nk=10\nkf = StratifiedKFold(n_splits=k)\nkf.get_n_splits(X, Y)\n\nall_result = []\nnames_methods = ['GaussianNB', 'BernoulliNB', 'RandomForestClassifier']\n# clf = GaussianNB()\n# clf = BernoulliNB()\n# clf = RandomForestClassifier(n_estimators=300, oob_score=True, random_state=123456)\n\nfor i,clf in enumerate([GaussianNB(), BernoulliNB(), RandomForestClassifier(n_estimators=300, oob_score=True, random_state=123456)]):\n\n    result=[]\n    print(names_methods[i])\n    print('---'*20)\n    print('KFold\\t|Acc\\t|Prec\\t|Rec\\t|F1\\t|')\n    for i,(train_index, test_index) in enumerate(kf.split(X,Y)):\n\n        X_train = X[train_index,:]; X_test = X[test_index,:]\n        y_train = Y[train_index  ]; y_test = Y[test_index  ]\n\n        # Estimate\n        clf.fit(X_train,y_train)\n        # Predict\n        y_test_hat = clf.predict(X_test)\n\n        # Evaluate\n        acc = metrics.accuracy_score(y_test, y_test_hat)\n        precision = metrics.precision_score(y_test, y_test_hat, average='macro')\n        recall = metrics.recall_score(y_test, y_test_hat, average='macro')\n        f1_score = 2*precision*recall/(precision+recall)\n\n        result.append([acc,precision,recall,f1_score])  \n        print( 'K({}):\\t|{:0.3f}\\t|{:0.3f}\\t|{:0.3f}\\t|{:0.3f}\\t|'.format(i,acc,precision,recall,f1_score).replace('.',',')  )\n    print('---'*20)\n    \n    all_result.append( np.stack(result, axis=1).T )\n    print(' ')\n    \n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2bad1fe8df71b850dfeee59f9baeefabeece92fc"
      },
      "cell_type": "code",
      "source": "\nMeans=[]\nStds=[]\nfor mat in all_result:\n    Means.append( mat.mean(axis=0) )\n    Stds.append( mat.std(axis=0) )\n\n    \nplt.figure(figsize=(12,5))\nind = np.arange(4) \nwidth = 0.35 \n\np1 = plt.bar(ind - width/3, Means[0], 0.35, yerr=Stds[0], color='SkyBlue')\np2 = plt.bar(ind, Means[1], 0.35, yerr=Stds[1], color='IndianRed')\np3 = plt.bar(ind + width/3, Means[2], 0.35, yerr=Stds[2], color='green')\n\nplt.ylabel('Scores')\nplt.title('Scores by group and gender')\nplt.xticks(ind, ('Acc', 'Prec', 'Rec', 'F1'))\nplt.legend((p1[0], p2[0], p3[0]), names_methods)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b2f5d0b7ca2201c9956b03fced2d910bebdd9502"
      },
      "cell_type": "markdown",
      "source": "## Beyond Good and Evil\nLet’s turn our problem into a regression task and try to predict the super-heroes weight given the other features.\n\n1. Which algorithm did you pick and why?\n\nI selected two features for test: Height and Gender. For categorical data, the best methods are the methods based on Decision Tree like Random Forest. As I selected two features I also testing with lineal model with SGD.\n\n2. How do you evaluate the performance of your algorithm in this case?\n\n\nLeave-One-Out cross-validator\n\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c2a51a90a81562d3af3d68479f9428b1403f337"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn.metrics as metrics\n\n# X = np.array(data_prep.drop( ['Weight'], axis=1 ).copy())\nX = np.array(data_prep[ ['Height', 'Gender'] ].copy())\nY = np.array(data_prep[ ['Weight'] ].copy())\n\n# Leave-One-Out cross-validator\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle=True, random_state=0)\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "658387638d151ac260d0221fff484bffd8f7c48a"
      },
      "cell_type": "code",
      "source": "# Import the model we are using\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model.stochastic_gradient import SGDRegressor\n\n# Instantiate model with 1000 decision trees\nrf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\nsg = SGDRegressor(penalty='elasticnet', alpha=0.01, l1_ratio=0.25, fit_intercept=True, tol=1e-4)\n\n# Train the model on training data\npoly = PolynomialFeatures(degree=2)\nrf.fit(poly.fit_transform(X_train), y_train);\nsg.fit(poly.fit_transform(X_train), y_train);\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b6409d2db58687df9fcbd59194ffd8fc255417a2"
      },
      "cell_type": "code",
      "source": "# Use the forest's predict method on the test data\npredictions_rf = rf.predict(poly.fit_transform(X_test))\npredictions_sg = sg.predict(poly.fit_transform(X_test))\n\n# Calculate the absolute errors\nerrors_rf = abs(predictions_rf - y_test)\nerrors_sg = abs(predictions_sg - y_test)\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error of RandomForestRegressor:', round(np.mean(errors_rf), 2), 'kg')\nprint('Mean Absolute Error of SGDRegressor:', round(np.mean(errors_sg), 2), 'kg')\n\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eeba351ea5cf2cc76877d24ec798312aac7ddc4e"
      },
      "cell_type": "code",
      "source": "colors = ['teal', 'yellowgreen', 'gold']\nx_plot = np.arange(len(y_test))\nlw = 2\n\nplt.figure(figsize=(22,5))\nplt.plot(x_plot, y_test, color='cornflowerblue', linewidth=lw, label=\"Actual\")\nplt.plot(x_plot, predictions_rf[:,np.newaxis], color='gold', linewidth=lw, label=\"Random Forest\")\nplt.plot(x_plot, predictions_sg[:,np.newaxis], color='teal', linewidth=lw, label=\"SGD\")\nplt.scatter(x_plot, y_test, color='navy', s=30, marker='o', label=\"Points\")\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9aa3066c2752f7e13e5dc056182deb4a29c8f682"
      },
      "cell_type": "markdown",
      "source": "## Bonus\n\nIf you enjoyed playing with the super-heroes dataset, this section is for you to showcase any further aspects of the data we have not explored in the questions. As a bonus section, this is totally optional, but we would love to see the insights you can get from this data. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "15746ad1ce5f6aab32fac18ee2cf17b1f110a387"
      },
      "cell_type": "markdown",
      "source": "# **Spectre is the most powerful !!!!! **\n## The Spectre is the name given to several fictional **antihero** characters who have appeared in numerous comic books published by DC Comics.\n\n![Spectre](https://upload.wikimedia.org/wikipedia/en/4/42/Spectre01.jpg)\n"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "57ad98014e7ac5d9fc9919a40f5c1ab5e590349c"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45f32b939dc2e5ef1a23983b8503d838a9e27231"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "414098cdd461f1ed181c5f07e1ef86c6a5bfdcae"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}